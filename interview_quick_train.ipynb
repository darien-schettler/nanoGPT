{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nanoGPT â€“ Quick Training Notebook\n",
    "\n",
    "Use this notebook to run a short training loop on Tiny Shakespeare (character-level).\n",
    "\n",
    "Workflow:\n",
    "1) Run \"Setup\" to detect device and branch.\n",
    "2) Run \"Prepare data\" once (creates `data/shakespeare_char/train.bin`, `val.bin`).\n",
    "3) Run \"Train\" to see losses (train/val) for a very short run.\n",
    "4) If you edit `model.py`, rerun the \"Reload model & Rebuild\" cell to pick up changes (module reload).\n",
    "\n",
    "Configuration:\n",
    "- Small model (4 layers, 4 heads, 128 embedding dim)\n",
    "- Fast training (300 iterations)\n",
    "- No DDP, no `torch.compile` for simplicity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Git branch: interview-kvcache-solution\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports, device, utils\n",
    "from __future__ import annotations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "# Ensure repository root on sys.path\n",
    "repo_root = Path.cwd()\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.append(str(repo_root))\n",
    "\n",
    "\n",
    "def detect_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "def current_branch() -> str:\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=repo_root\n",
    "        )\n",
    "        return out.decode().strip()\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def checkout_branch(name: str) -> None:\n",
    "    \"\"\"Optionally switch branches from within the notebook, then reload model in a later cell.\"\"\"\n",
    "    subprocess.check_call([\"git\", \"checkout\", name], cwd=repo_root)\n",
    "    print(f\"Switched to branch: {name}\")\n",
    "\n",
    "\n",
    "device = detect_device()\n",
    "torch.manual_seed(1337)\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Git branch: {current_branch()}\")\n",
    "\n",
    "# Plot style\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny Shakespeare binaries already exist.\n",
      "- /Users/darienschettler/PycharmProjects/nanoGPT-1/data/shakespeare_char/train.bin\n",
      "- /Users/darienschettler/PycharmProjects/nanoGPT-1/data/shakespeare_char/val.bin\n"
     ]
    }
   ],
   "source": [
    "# Package import (now that we've updated sys.path)\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# Prepare data: Tiny Shakespeare (character-level)\n",
    "DATA_DIR = repo_root / \"data\" / \"shakespeare_char\"\n",
    "TRAIN_BIN = DATA_DIR / \"train.bin\"\n",
    "VAL_BIN = DATA_DIR / \"val.bin\"\n",
    "\n",
    "if TRAIN_BIN.exists() and VAL_BIN.exists():\n",
    "    print(\"Tiny Shakespeare binaries already exist.\")\n",
    "    print(f\"- {TRAIN_BIN}\")\n",
    "    print(f\"- {VAL_BIN}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Preparing Tiny Shakespeare dataset (this downloads ~1MB and preprocesses)...\"\n",
    "    )\n",
    "    subprocess.check_call([sys.executable, str(DATA_DIR / \"prepare.py\")], cwd=repo_root)\n",
    "    assert TRAIN_BIN.exists() and VAL_BIN.exists(), (\n",
    "        \"Data preparation failed to produce train.bin/val.bin\"\n",
    "    )\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities: tiny config & dataloader\n",
    "# Small, fast config suitable for CPU/MPS/GPU\n",
    "block_size = 512\n",
    "batch_size = 16\n",
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_embd = 128\n",
    "dropout = 0.0\n",
    "max_iters = 5_000\n",
    "log_interval = 10\n",
    "eval_interval = 100\n",
    "eval_iters = 25\n",
    "learning_rate = 3e-4\n",
    "weight_decay = 0.1\n",
    "\n",
    "# Use KV-cache during sampling/generation\n",
    "use_kv_cache_in_generation = True\n",
    "\n",
    "# Dataloader: same shape semantics as train.py\n",
    "DATA_MEM_TRAIN = str(TRAIN_BIN)\n",
    "DATA_MEM_VAL = str(VAL_BIN)\n",
    "\n",
    "\n",
    "def get_batch(split: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    data = np.memmap(\n",
    "        DATA_MEM_TRAIN if split == \"train\" else DATA_MEM_VAL, dtype=np.uint16, mode=\"r\"\n",
    "    )\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack(\n",
    "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
    "    )\n",
    "    y = torch.stack(\n",
    "        [\n",
    "            torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64))\n",
    "            for i in ix\n",
    "        ]\n",
    "    )\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model) -> dict[str, float]:\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss, _ = model(\n",
    "                X, Y\n",
    "            )  # Unpack 3 values (cache is None during training)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model from: /Users/darienschettler/PycharmProjects/nanoGPT-1/model.py\n",
      "number of parameters: 0.80M\n",
      "Model built.\n"
     ]
    }
   ],
   "source": [
    "# Reload model & Rebuild (run this after switching branches or editing model.py)\n",
    "import importlib\n",
    "import model as model_module\n",
    "\n",
    "importlib.reload(model_module)\n",
    "print(f\"Using model from: {model_module.__file__}\")\n",
    "\n",
    "# Infer vocab size from dataset meta if present, fallback to GPT-2 padded size\n",
    "meta_path = DATA_DIR / \"meta.pkl\"\n",
    "meta_vocab_size = None\n",
    "if meta_path.exists():\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = int(meta.get(\"vocab_size\", 50304))\n",
    "\n",
    "vocab_size = meta_vocab_size if meta_vocab_size is not None else 50304\n",
    "\n",
    "cfg = GPTConfig(\n",
    "    block_size=block_size,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    dropout=dropout,\n",
    "    bias=True,\n",
    ")\n",
    "model = GPT(cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "print(\"Model built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] train loss: 4.2140, val loss: 4.2093\n",
      "iter   10 | loss 3.4049\n",
      "iter   20 | loss 3.1388\n",
      "iter   30 | loss 2.9821\n",
      "iter   40 | loss 2.8805\n",
      "iter   50 | loss 2.7761\n",
      "iter   60 | loss 2.7256\n",
      "iter   70 | loss 2.6909\n",
      "iter   80 | loss 2.6699\n",
      "iter   90 | loss 2.6287\n",
      "iter  100 | loss 2.6179\n",
      "[eval] iter  100 | train 2.6147 | val 2.6215\n",
      "iter  110 | loss 2.6159\n",
      "iter  120 | loss 2.6061\n",
      "iter  130 | loss 2.5671\n",
      "iter  140 | loss 2.5651\n",
      "iter  150 | loss 2.5595\n",
      "iter  160 | loss 2.5575\n",
      "iter  170 | loss 2.5649\n",
      "iter  180 | loss 2.5283\n",
      "iter  190 | loss 2.5323\n",
      "iter  200 | loss 2.5319\n",
      "[eval] iter  200 | train 2.5178 | val 2.5337\n",
      "iter  210 | loss 2.5039\n",
      "iter  220 | loss 2.5406\n",
      "iter  230 | loss 2.5027\n",
      "iter  240 | loss 2.5025\n",
      "iter  250 | loss 2.4771\n",
      "iter  260 | loss 2.5276\n",
      "iter  270 | loss 2.4747\n",
      "iter  280 | loss 2.4890\n",
      "iter  290 | loss 2.4981\n",
      "iter  300 | loss 2.4728\n",
      "[eval] iter  300 | train 2.4854 | val 2.4822\n",
      "iter  310 | loss 2.4728\n",
      "iter  320 | loss 2.4891\n",
      "iter  330 | loss 2.4864\n",
      "iter  340 | loss 2.4599\n"
     ]
    }
   ],
   "source": [
    "# Train (brief run) and plot\n",
    "train_losses, val_losses, steps = [], [], []\n",
    "\n",
    "# Initial eval\n",
    "losses = estimate_loss(model)\n",
    "print(f\"[init] train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
    "train_losses.append(losses[\"train\"])\n",
    "val_losses.append(losses[\"val\"])\n",
    "steps.append(0)\n",
    "\n",
    "for it in range(1, max_iters + 1):\n",
    "    X, Y = get_batch(\"train\")\n",
    "    # Note: KV-cache is NOT used during training (only during generation)\n",
    "    # Training always processes full sequences, so cache provides no benefit\n",
    "    logits, loss, _ = model(X, Y)  # kv_cache=None by default\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if it % log_interval == 0:\n",
    "        print(f\"iter {it:4d} | loss {loss.item():.4f}\")\n",
    "\n",
    "    if it % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(\n",
    "            f\"[eval] iter {it:4d} | train {losses['train']:.4f} | val {losses['val']:.4f}\"\n",
    "        )\n",
    "        train_losses.append(losses[\"train\"])\n",
    "        val_losses.append(losses[\"val\"])\n",
    "        steps.append(it)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(steps, train_losses, label=\"train\")\n",
    "plt.plot(steps, val_losses, label=\"val\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(f\"Branch: {current_branch()} | device: {device}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sampling helper after training\n",
    "def sample(\n",
    "    model,\n",
    "    start: str = \"To be, or\",\n",
    "    max_new_tokens: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    use_kv_cache: bool = True,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    # Load tokenizer metadata if present (char-level uses simple identity)\n",
    "    # For Tiny Shakespeare char-level, meta.pkl includes 'itos' / 'stoi'\n",
    "    import pickle\n",
    "\n",
    "    meta_path = DATA_DIR / \"meta.pkl\"\n",
    "    if not meta_path.exists():\n",
    "        print(\"No meta.pkl found; sampling skipped.\")\n",
    "        return \"\"\n",
    "    with open(meta_path, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    stoi = meta[\"stoi\"]\n",
    "    itos = meta[\"itos\"]\n",
    "\n",
    "    def encode(s: str):\n",
    "        return [stoi[c] for c in s]\n",
    "\n",
    "    def decode(ids):\n",
    "        return \"\".join([itos[i] for i in ids])\n",
    "\n",
    "    idx = torch.tensor([encode(start)], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        # Use KV-cache for faster generation if enabled\n",
    "        idx = model.generate(\n",
    "            idx,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            use_cache=use_kv_cache,\n",
    "        )\n",
    "    return decode(idx[0].tolist())\n",
    "\n",
    "TOKENS_TO_GENERATE = 500\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"WITH CACHE:\\n{'=' * 80}\\n\")\n",
    "t1 = time.time()\n",
    "print(sample(model, start=\"ROMEO:\", max_new_tokens=TOKENS_TO_GENERATE, temperature=TEMPERATURE, use_kv_cache=use_kv_cache_in_generation))\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(time.time() - t1)\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\\n\\n{'=' * 80}\\nWITHOUT CACHE:\\n{'=' * 80}\\n\")\n",
    "t1 = time.time()\n",
    "print(sample(model, start=\"ROMEO:\", max_new_tokens=TOKENS_TO_GENERATE, temperature=TEMPERATURE, use_kv_cache=False))\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(time.time() - t1)\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
