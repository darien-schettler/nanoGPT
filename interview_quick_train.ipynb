{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# nanoGPT â€“ Quick Training Notebook for Interview\n",
        "\n",
        "Use this notebook to run a short training loop on Tiny Shakespeare (character-level) to validate behavior across branches:\n",
        "- master (expected to train normally)\n",
        "- bug/1-broken-attention-mask\n",
        "- bug/2-missing-residual-connection\n",
        "- bug/3-missing-positional-embedding\n",
        "\n",
        "Workflow:\n",
        "1) Run \"Setup\" to detect device and branch.\n",
        "2) Run \"Prepare data\" once (creates `data/shakespeare_char/train.bin`, `val.bin`).\n",
        "3) Run \"Train\" to see losses (train/val) for a very short run.\n",
        "4) Switch branches (either in your IDE/terminal or using the helper below), then run \"Reload model & Rebuild\" and \"Train\" again.\n",
        "\n",
        "Tip:\n",
        "- If you switch branches or edit `model.py`, rerun the \"Reload model & Rebuild\" cell to pick up changes (module reload).\n",
        "- Designed to be simple and fast; no DDP, no `torch.compile`, tiny model, short run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: imports, device, utils\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure repository root on sys.path\n",
        "repo_root = Path.cwd()\n",
        "if str(repo_root) not in sys.path:\n",
        "    sys.path.append(str(repo_root))\n",
        "\n",
        "\n",
        "def detect_device() -> str:\n",
        "    if torch.cuda.is_available():\n",
        "        return 'cuda'\n",
        "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        return 'mps'\n",
        "    return 'cpu'\n",
        "\n",
        "\n",
        "def current_branch() -> str:\n",
        "    try:\n",
        "        out = subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=repo_root)\n",
        "        return out.decode().strip()\n",
        "    except Exception:\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "def checkout_branch(name: str) -> None:\n",
        "    \"\"\"Optionally switch branches from within the notebook, then reload model in a later cell.\"\"\"\n",
        "    subprocess.check_call([\"git\", \"checkout\", name], cwd=repo_root)\n",
        "    print(f\"Switched to branch: {name}\")\n",
        "\n",
        "\n",
        "device = detect_device()\n",
        "torch.manual_seed(1337)\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Git branch: {current_branch()}\")\n",
        "\n",
        "# Plot style\n",
        "plt.style.use('ggplot')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data: Tiny Shakespeare (character-level)\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "DATA_DIR = repo_root / \"data\" / \"shakespeare_char\"\n",
        "TRAIN_BIN = DATA_DIR / \"train.bin\"\n",
        "VAL_BIN = DATA_DIR / \"val.bin\"\n",
        "\n",
        "if TRAIN_BIN.exists() and VAL_BIN.exists():\n",
        "    print(\"Tiny Shakespeare binaries already exist.\")\n",
        "    print(f\"- {TRAIN_BIN}\")\n",
        "    print(f\"- {VAL_BIN}\")\n",
        "else:\n",
        "    print(\"Preparing Tiny Shakespeare dataset (this downloads ~1MB and preprocesses)...\")\n",
        "    subprocess.check_call([sys.executable, str(DATA_DIR / \"prepare.py\")], cwd=repo_root)\n",
        "    assert TRAIN_BIN.exists() and VAL_BIN.exists(), \"Data preparation failed to produce train.bin/val.bin\"\n",
        "    print(\"Done.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training utilities: tiny config & dataloader\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Small, fast config suitable for CPU/MPS/GPU\n",
        "block_size = 64\n",
        "batch_size = 16\n",
        "n_layer = 4\n",
        "n_head = 4\n",
        "n_embd = 128\n",
        "dropout = 0.0\n",
        "max_iters = 300\n",
        "log_interval = 10\n",
        "eval_interval = 50\n",
        "eval_iters = 20\n",
        "learning_rate = 3e-4\n",
        "weight_decay = 0.1\n",
        "\n",
        "# Dataloader: same shape semantics as train.py\n",
        "DATA_MEM_TRAIN = str(TRAIN_BIN)\n",
        "DATA_MEM_VAL = str(VAL_BIN)\n",
        "\n",
        "def get_batch(split: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    data = np.memmap(DATA_MEM_TRAIN if split == 'train' else DATA_MEM_VAL, dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model) -> dict[str, float]:\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean().item()\n",
        "    model.train()\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload model & Rebuild (run this after switching branches or editing model.py)\n",
        "import importlib\n",
        "import model as model_module\n",
        "importlib.reload(model_module)\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "print(f\"Using model from: {model_module.__file__}\")\n",
        "\n",
        "# Infer vocab size from dataset meta if present, fallback to GPT-2 padded size\n",
        "import pickle\n",
        "meta_path = DATA_DIR / 'meta.pkl'\n",
        "meta_vocab_size = None\n",
        "if meta_path.exists():\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    meta_vocab_size = int(meta.get('vocab_size', 50304))\n",
        "\n",
        "vocab_size = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "\n",
        "cfg = GPTConfig(\n",
        "    block_size=block_size,\n",
        "    vocab_size=vocab_size,\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        "    dropout=dropout,\n",
        "    bias=True,\n",
        ")\n",
        "model = GPT(cfg).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "print(\"Model built.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train (brief run) and plot\n",
        "train_losses, val_losses, steps = [], [], []\n",
        "\n",
        "# Initial eval\n",
        "losses = estimate_loss(model)\n",
        "print(f\"[init] train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
        "train_losses.append(losses['train'])\n",
        "val_losses.append(losses['val'])\n",
        "steps.append(0)\n",
        "\n",
        "for it in range(1, max_iters + 1):\n",
        "    X, Y = get_batch('train')\n",
        "    logits, loss = model(X, Y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if it % log_interval == 0:\n",
        "        print(f\"iter {it:4d} | loss {loss.item():.4f}\")\n",
        "\n",
        "    if it % eval_interval == 0:\n",
        "        losses = estimate_loss(model)\n",
        "        print(f\"[eval] iter {it:4d} | train {losses['train']:.4f} | val {losses['val']:.4f}\")\n",
        "        train_losses.append(losses['train'])\n",
        "        val_losses.append(losses['val'])\n",
        "        steps.append(it)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(steps, train_losses, label='train')\n",
        "plt.plot(steps, val_losses, label='val')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.title(f\"Branch: {current_branch()} | device: {device}\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: quick sampling helper after training\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def sample(model, start: str = \"To be, or\", max_new_tokens: int = 100, temperature: float = 1.0) -> str:\n",
        "    model.eval()\n",
        "    # Load tokenizer metadata if present (char-level uses simple identity)\n",
        "    # For Tiny Shakespeare char-level, meta.pkl includes 'itos' / 'stoi'\n",
        "    import pickle\n",
        "    meta_path = DATA_DIR / 'meta.pkl'\n",
        "    if not meta_path.exists():\n",
        "        print(\"No meta.pkl found; sampling skipped.\")\n",
        "        return \"\"\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    stoi = meta['stoi']\n",
        "    itos = meta['itos']\n",
        "\n",
        "    def encode(s: str):\n",
        "        return [stoi[c] for c in s]\n",
        "\n",
        "    def decode(ids):\n",
        "        return ''.join([itos[i] for i in ids])\n",
        "\n",
        "    idx = torch.tensor([encode(start)], dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        idx = model.generate(idx, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "print(sample(model, start=\"ROMEO:\", max_new_tokens=200, temperature=0.8))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
